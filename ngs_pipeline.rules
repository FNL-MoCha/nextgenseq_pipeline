import itertools
import os
import collections
import json
import glob
from snakemake.utils import R
from snakemake.utils import min_version
min_version("3.2")
from snakemake.exceptions import MissingInputException
# Snakemake Base location
NGS_PIPELINE=os.environ['NGS_PIPELINE']
WORK_DIR=os.environ['WORK_DIR']
DATA_DIR=os.environ['DATA_DIR']
ACT_DIR=os.environ['ACT_DIR']
HOST=os.environ['HOST']
TIME=os.environ['TIME']
NOW=os.environ['TMP']
configfile: NGS_PIPELINE +"/config/config_annotation.json"
configfile: NGS_PIPELINE +"/config/config_common.json"
configfile: NGS_PIPELINE +"/config/config_cluster.json"
if HOST == 'biowulf.nih.gov':
	configfile: NGS_PIPELINE +"/config/config_common_biowulf.json"
elif HOST == 'login01':
	configfile: NGS_PIPELINE +"/config/config_common_tgen.json"
elif HOST == 'moab':
	configfile: NGS_PIPELINE +"/config/config_common_moab.json"

config['host'] = HOST
GROUP=config['group']
VERSION=config["pipeline_version"]
MAIL=config['mail']
#HOST = config['host']
###########################################################################
#
#		This initializes all the variables we need for the jobs.
#		It also removes the host specific constraints like scratch
#		area on the node.
#		module purge is needed to remove all the loaded modules and
#			inside the rule load what is necessary.
###########################################################################
shell.prefix("""
set -e -o pipefail
sleep 20s
MEM=`echo "${{SLURM_MEM_PER_NODE}} / 1024 "|bc`
LOCAL="/lscratch/${{SLURM_JOBID}}/"
THREADS=${{SLURM_CPUS_ON_NODE}}
""")
#module purge
#if [ {HOST} == 'biowulf.nih.gov' ]
#	then
#		MEM=`echo "${{SLURM_MEM_PER_NODE}} / 1024 "|bc`
#		LOCAL="/lscratch/${{SLURM_JOBID}}/"
#		THREADS=${{SLURM_CPUS_ON_NODE}}
#elif [ {HOST} == 'login01' ]
#	then
#		module load slurm
#		module load gcc/4.8.1
#		MEM=`scontrol show job ${{SLURM_JOB_ID}} | grep "MinMemoryNode"| perl -n -e'/MinMemoryNode=(\d*)G/ && print $1'`
#		mkdir -p /projects/scratch/ngs_pipeline_{NOW}_${{SLURM_JOB_ID}}/
#		LOCAL="/projects/scratch/ngs_pipeline_{NOW}_${{SLURM_JOB_ID}}/"
#		THREADS=`scontrol show job ${{SLURM_JOB_ID}} | grep  "MinCPUsNode" | perl -n -e'/MinCPUsNode=(\d*)/ && print $1'`
#fi
#""")
###########################################################################
#
#			Conversion
#
###########################################################################
SUBJECT_TO_SAMPLE  = {}
for subject in config['DNASeq']:
	SUBJECT_TO_SAMPLE[subject] = expand("{sample}", sample = config['DNASeq'][subject])
###########################################################################
SAMPLE_TO_SUBJECT  = {}
for subject,samples in config['DNASeq'].items():
	for sample in samples:
		SAMPLE_TO_SUBJECT[sample]=subject
###########################################################################
#make dictionary containing fastq file locations.
# Die if a library is ran twice or more.
FQ={}
for sample in config['library'].keys():
	for fq in config['library'][sample]:
		if len(config['library'][sample]) == 1:
			if os.path.isfile(DATA_DIR+fq+"/"+fq+"_R1.fastq.gz"):
				FQ[sample] =[DATA_DIR+fq+"/"+fq+"_R1.fastq.gz", DATA_DIR+fq+"/"+fq+"_R2.fastq.gz"]
			elif os.path.isfile(DATA_DIR+"Sample_"+fq+"/Sample_"+fq+"_R1.fastq.gz"):
				FQ[sample] =[DATA_DIR+"Sample_"+fq+"/Sample_"+fq+"_R1.fastq.gz", DATA_DIR+"Sample_"+fq+"/Sample_"+fq+"_R2.fastq.gz"]
			elif os.path.exists(DATA_DIR+fq):
				R1=glob.glob(DATA_DIR+fq)
				FQ[sample] =[]
				FQ[sample].extend(R1)
			elif os.path.exists(DATA_DIR+"Sample_"+fq):
				R1=glob.glob(DATA_DIR+"Sample_"+fq)
				FQ[sample] =[]
				FQ[sample].extend(R1)
			else:
				print("#####################################")
				print("")
				print("")
				print("Can not find fastq files for", sample)
				print("")
				print("")
				print("#####################################")
				exit()
		else:
			print("#####################################")
			print("")
                        print("")
			print("Can not process fastq files from two locations for ",sample )
			print("")
			print("")
			print("#####################################")
			exit()
###########################################################################
####
#### Targets
####
PATIENTS =[]
SUB_BAMS= {}
SUB_COV = {}
SUB_LOH = {}
SUB_GT  = {}
SUB_HOT = {}
SAMPLES =[]
somaticPairs = {}
pairedCapture = {}
# Inputs for the targets, where direct list can not be used.
for subject in config['DNASeq'].keys():
	PATIENTS.append(subject)
	SUB_BAMS[subject]= ["{subject}/{TIME}/{sample}/{sample}.bwa.final.bam".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['DNASeq'][subject]]
	SUB_COV[subject] = ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.coverage.txt".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['DNASeq'][subject]]
	SUB_HOT[subject] = ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.hotspot.depth".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['DNASeq'][subject]]
	SUB_LOH[subject] = ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.loh".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['DNASeq'][subject]]
	SUB_GT[subject]  = ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.gt".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['DNASeq'][subject]]
	for sample in config['DNASeq'][subject]:
		SAMPLES.append(sample)
###########################################################################
#		Add RNASeq only samples to PATIENTS
###########################################################################
for subject in config['RNASeq']:
	SUBJECT_TO_SAMPLE[subject] = expand("{sample}", sample = config['RNASeq'][subject])
for subject  in config['RNASeq'].keys():
        if subject not in PATIENTS:
                PATIENTS.append(subject)
###########################################################################
# Many of the targets.
TARGET      = ["{subject}/{TIME}/{sample}/qc/fastqc/{sample}_R2_fastqc.html".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.flagstat.txt".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += ["{subject}/{TIME}/{sample}/{sample}.bwa.final.bam".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.hotspot.depth".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.gt".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += ["{subject}/{TIME}/{sample}/qc/{sample}.gender".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += expand("{subject}/{TIME}/qc/{subject}.genotyping.txt", TIME=TIME, subject=PATIENTS)
TARGET     += expand("{subject}/{TIME}/annotation/AnnotationInput.coding.rare.txt", TIME=TIME, subject=PATIENTS)
TARGET     += expand("{subject}/{TIME}/annotation/{subject}.Annotations.coding.rare.txt", TIME=TIME, subject=PATIENTS)
#TARGET     += expand("{subject}/{TIME}/igv/session_{subject}.xml", TIME=TIME, subject=PATIENTS)

if len(config['matched_normal']) > 0:
	for Tumor in config['matched_normal']:
		for Normal in config['matched_normal'][Tumor]:
			TumorBam   = "{subject}/{TIME}/{sample}/{sample}.bwa.final".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)
			NormalBam  = "{subject}/{TIME}/{sample}/{sample}.bwa.final".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[Normal], sample=Normal)
			pairedCapture[Tumor] = config['sample_captures'][Tumor]
			somaticPairs[Tumor] = [TumorBam + ".bam" , TumorBam + ".bam.bai", NormalBam + ".bam", NormalBam + ".bam.bai"]
###########################################################################
# This is to make list of DB file list. (germline, variants, somatic, rnaseq)
SUBJECT_ANNO = dict([(key, {}) for key in PATIENTS])
def add_to_SUBJECT_ANNO(subject, category, file_list):
	if category not in SUBJECT_ANNO[subject]:
		SUBJECT_ANNO[subject][category] = file_list
	else:
		SUBJECT_ANNO[subject][category].extend(file_list)
###########################################################################
SUBJECT_VCFS = {}
SOMATIC =[]
###########################################################################
# This is to find out if we need to make variants db file or germline file.
ACT_TYPE =[]
DECIDE_GL={}
for subject in config['DNASeq'].keys():
	normal = None
	tumor  = None
	pair   = None
	for sample in config['DNASeq'][subject]:
		if config['sample_type'][sample] == 'Tumor':
			tumor = 'yes'
			if sample in config['matched_normal'].keys():
				pair  = 'yes'
		elif config['sample_type'][sample] == 'Normal':
			normal = 'yes'
	if pair =='yes':
		DECIDE_GL[subject] = 'gl_only'
	elif pair == None:
		if tumor == None and normal =='yes':
			DECIDE_GL[subject] = 'gl_only'
		else:
			ACT_TYPE +=[subject]
##########################################################################
# To create lists to be filled in SUBJECT_ANNO
for subject in config['DNASeq']:
	local  = []
	for sample in config['DNASeq'][subject]:
		local.extend([(subject+"/"+TIME+"/"+sample+"/calls/"+sample+".HC_DNASeq.snpEff.txt"),
			      (subject+"/"+TIME+"/"+sample+"/calls/"+sample+".Platypus.snpEff.txt")
			    ])
	if subject not in SUBJECT_VCFS:
		SUBJECT_VCFS[subject] = local
	if subject in ACT_TYPE:
		germline = [w.replace('snpEff','annotated') for w in local]
		add_to_SUBJECT_ANNO(subject,"variants",germline)
	else:
		germline = [w.replace('snpEff','annotated') for w in local]
		add_to_SUBJECT_ANNO(subject,"germline",germline)
	TARGET.extend(local)	
for sample in config['matched_normal'].keys():
	local  = []
	subject=SAMPLE_TO_SUBJECT[sample]
	TARGET += [subject+"/"+TIME+"/"+sample+"/calls/"+sample+".MuTect.maf"],
	TARGET += [subject+"/"+TIME+"/"+sample+"/calls/"+sample+".MuTect2.maf"],
	local.extend(
		[ (subject+"/"+TIME+"/"+sample+"/calls/"+sample+".MuTect.snpEff.txt"),
		  (subject+"/"+TIME+"/"+sample+"/calls/"+sample+".strelka.snvs.snpEff.txt"),
		  (subject+"/"+TIME+"/"+sample+"/calls/"+sample+".strelka.indels.snpEff.txt")
		]
	)
	SOMATIC     +=[subject+"/"+TIME+"/"+sample+"/calls/"+sample+".MuTect.annotated.txt"]
	SOMATIC     +=[subject+"/"+TIME+"/"+sample+"/calls/"+sample+".strelka.snvs.annotated.txt"]
	SOMATIC     +=[subject+"/"+TIME+"/"+sample+"/calls/"+sample+".strelka.indels.annotated.txt"]
	TARGET.extend(SOMATIC)
	if subject in SUBJECT_VCFS:
		SUBJECT_VCFS[subject].extend(local)
	somatic = [w.replace('snpEff','annotated') for w in local]
	if sample in config['matched_rnaseq']:
		somatic = [w.replace('MuTect.annotated','MuTect.annotated.expressed') for w in somatic]
		somatic = [w.replace('strelka.snvs.annotated','strelka.snvs.annotated.expressed') for w in somatic]
		somatic = [w.replace('strelka.indels.annotated','strelka.indels.annotated.expressed') for w in somatic]
	add_to_SUBJECT_ANNO(subject,"somatic",somatic)
###########################################################################
###########################################################################
# Expressed Mutations
expressedPairs = {}
if 'matched_rnaseq' in config:
	if len(config['matched_rnaseq']) > 0:
		for Tumor in config['matched_rnaseq']:
			if Tumor in config['matched_normal'].keys():
				for RNASample in config['matched_rnaseq'][Tumor]:
					subject=SAMPLE_TO_SUBJECT[Tumor]
					RNASeqBam    = subject + "/"+TIME+ "/" + RNASample + "/calls/"+RNASample + ".HC_RNASeq.snpEff.txt"
					expressedPairs[Tumor] = RNASeqBam
					TARGET += ["{subject}/{TIME}/{sample}/calls/{sample}.MuTect.annotated.expressed.txt".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[Tumor],  sample=Tumor)]
					TARGET += ["{subject}/{TIME}/{sample}/calls/{sample}.strelka.snvs.annotated.expressed.txt".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)]
					TARGET += ["{subject}/{TIME}/{sample}/calls/{sample}.strelka.indels.annotated.expressed.txt".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)]
			# Its possible to have Matched RNASeq, i.e. Tumor only sequencing (Exome/RNA) but missing Corrosponding Normal
			#print(Tumor, "Corrosponding RNASeq found but matched Normal not found")
###########################################################################
localrules: Pipeline, RNASeq, IGV_Session, DBinput, AttachAnnotation, Actionable_Germline, Actionable_RNAseq, Actionable_Somatic, Actionable_Variants,  makeConfig, QC_Summary_Patient,QC_Summary,UnionSomaticCalls,TOPHAT_LINK, SampleGT,QC_Sum, FormatInput, RNASeqQC2,RNASeqQC3, Cuff_Mat
#Circos, CoveragePlot, BoxPlot_Hotspot, makeConfig,Ideogram
###########################################################################
#				Include Rule Book			  #
###########################################################################
include: NGS_PIPELINE +"/ruleBook/oncoKB.annotator.rules"
include: NGS_PIPELINE +"/ruleBook/MAGGIE.rules"
include: NGS_PIPELINE +"/ruleBook/ROH.rules"
include: NGS_PIPELINE +"/ruleBook/variantsHeatmap.rules"
include: NGS_PIPELINE +"/ruleBook/MSI.rules"
#include: NGS_PIPELINE +"/ruleBook/MANTIS.rules"
include: NGS_PIPELINE +"/ruleBook/Ancestry.rules"
include: NGS_PIPELINE +"/ruleBook/mergeHC_PLAT.rules"
include: NGS_PIPELINE +"/ruleBook/verifyBamID.rules"
include: NGS_PIPELINE +"/ruleBook/rnaseq_pipeline.rules"
include: NGS_PIPELINE +"/ruleBook/rnaseqQC.rules"
include: NGS_PIPELINE +"/ruleBook/readDepth.rules"
include: NGS_PIPELINE +"/ruleBook/failedExon.rules"
include: NGS_PIPELINE +"/ruleBook/hsMetrix.rules"
include: NGS_PIPELINE +"/ruleBook/Consolidate.rules"
#include: NGS_PIPELINE +"/ruleBook/universal.rules"
include: NGS_PIPELINE +"/ruleBook/mutationalSignature.rules"
#include: NGS_PIPELINE +"/ruleBook/NeoAntigen.rules"
include: NGS_PIPELINE +"/ruleBook/haplotypeCaller.rules"
include: NGS_PIPELINE +"/ruleBook/platypus.rules"
#include: NGS_PIPELINE +"/ruleBook/bam2mpg.rules"
include: NGS_PIPELINE +"/ruleBook/gatk_RNASeq.rules"
include: NGS_PIPELINE +"/ruleBook/ideogram.rules"
include: NGS_PIPELINE +"/ruleBook/Actionable.rules"
include: NGS_PIPELINE +"/ruleBook/UnionSomaticMutations.rules"
include: NGS_PIPELINE +"/ruleBook/plots.rules"
include: NGS_PIPELINE +"/ruleBook/annot.rules"
include: NGS_PIPELINE +"/ruleBook/featureCount.rules"
#include: NGS_PIPELINE +"/ruleBook/Sequenza.rules"
include: NGS_PIPELINE +"/ruleBook/cnvkit.rules"
include: NGS_PIPELINE +"/ruleBook/MethlySeq.rules"
include: NGS_PIPELINE +"/ruleBook/RSEM.rules"
#include: NGS_PIPELINE +"/ruleBook/HotSpot_Pileup.rules"
#include: NGS_PIPELINE +"/ruleBook/PyClone.rules"
###########################################################################
# we have to do it this way as some samples may not have rna or tumor     #
###########################################################################
for subject in SUBJECT_ANNO.keys():
	for group in SUBJECT_ANNO[subject].keys():
		#TARGET +=[subject+"/"+TIME+"/"+subject+"/db/"+subject+"."+group]
		#TARGET +=[subject+"/"+TIME+ACT_DIR+subject+"."+group+".actionable.txt"]
		for varFile in SUBJECT_ANNO[subject][group]:
			TARGET.append(varFile)
###########################################################################



for subject in SUBJECT_VCFS.keys():
	for vcf in SUBJECT_VCFS[subject]:
		vcf = vcf.replace('snpEff.txt', 'raw.vcf')
		TARGET +=[vcf]
		vcf = vcf.replace('raw.vcf', 'raw.snpEff.vcf')
		TARGET +=[vcf]
###########################################################################
onerror:
	shell("find .snakemake/ ! -readable -prune \( -type f -user $USER -exec chmod g+rw {{}} \; \) , \( -type d -user $USER -exec chmod g+rwx {{}} \; \)")
        shell("find .snakemake/ ! -readable -prune -group $USER -exec chgrp -f {GROUP} {{}} \;")
	shell("find {PATIENTS} -group $USER -exec chgrp -f {GROUP} {{}} \;")
	shell("find {PATIENTS} \( -type f -user $USER -exec chmod g+rw {{}} \; \) , \( -type d -user $USER -exec chmod g+rwx {{}} \; \)")
	shell("ssh -q {HOST} \"echo 'ngs-pipeline version {VERSION} failed on {PATIENTS}. Error occured on {HOST}. Working Dir:  {WORK_DIR}' |mutt -s ' ngs-pipeline Status' `whoami`@mail.nih.gov \"")
	shell("find .snakemake/ ! -readable -prune -group $USER -exec chgrp -f {GROUP} {{}} \;")
onstart:
	f = open('ngs_pipeline_%s.csv' % NOW , 'w')
	print ('#Patient','diagnosis','CaseID',sep='\t', end='\n',file=f)
	for subject in sorted(PATIENTS):
		diagnosis =config['diagnosis'][subject]
		print (subject,diagnosis,TIME,sep='\t', end='\n',file=f)
	
	shell("for sub in {PATIENTS}; do rm -rf {WORK_DIR}/${{sub}}/{TIME}/successful.txt ; done")
	shell("ssh -q {HOST} \"echo 'ngs-pipeline version {VERSION} started on {PATIENTS} on {HOST}. Working Dir:  {WORK_DIR}' |mutt -s ' ngs-pipeline Status' `whoami`@mail.nih.gov \"")
onsuccess:
	shell("find .snakemake/ ! -readable -prune \( -type f -user $USER -exec chmod g+r {{}} \; \) , \( -type d -user $USER -exec chmod g+rwx {{}} \; \)")
	shell("find .snakemake/ ! -readable -prune -group $USER -exec chgrp -f {GROUP} {{}} \;")
	print("Workflow finished, no error")
###########################################################################
rule Pipeline:
	input:
		TARGET,
		expand ("ngs_pipeline_{NOW}.rnaseq.done", NOW=NOW)
	version: config["pipeline_version"]
	wildcard_constraints:
		NOW="\w+"
	params:
		rulename = "Final",
		batch    = config[config['host']]["job_default"],
		group    = config["group"],
		mail 	 = NGS_PIPELINE + "/scripts/tsv2html.final.sh",
		email    = config["mail"],
		host     = config["host"],
		subs     = PATIENTS
	shell: """
	#######################
	rm -rf ngs_pipeline_{NOW}.rnaseq.done
	find {PATIENTS} log -group $USER -exec chgrp -f {params.group} {{}} \;
	find {PATIENTS} log \( -type f -user $USER -exec chmod g+rw {{}} \; \) , \( -type d -user $USER -exec chmod g+rwx {{}} \; \)
	export LC_ALL=C
	
	for sub in {params.subs}
        do
                touch {WORK_DIR}/${{sub}}/{TIME}/successful.txt
		chmod g+rw {WORK_DIR}/${{sub}}/{TIME}/successful.txt 	
		chgrp {params.group} {WORK_DIR}/${{sub}}/{TIME}/successful.txt
        done
	ssh -q {params.host} "{params.mail} --location {WORK_DIR} --host {params.host} --head --version {version} {WORK_DIR}/ngs_pipeline_{NOW}.csv |mutt -e \\\"my_hdr Content-Type: text/html\\\" -s ' ngs-pipeline Status' `whoami`@mail.nih.gov"
	rm -rf {WORK_DIR}/ngs_pipeline_{NOW}.csv
	#######################
	"""
############
# Print Config to a file
############
rule makeConfig:
	output:	"{subject}/{TIME}/qc/{subject}.config.txt" 
	params:
		rulename = "configPrint",
		batch    = config[config['host']]["job_default"],
		hash     = json.dumps(config, sort_keys=True)
	shell: """
	#######################
	echo '{params.hash}'  >{output}
	#######################
	"""
############
#       Merge Fastq Files
############
rule bbsplit:
	input: 
		R=lambda wildcards: FQ[wildcards.sample],
		ref=config['bbsplitIndex']
	output:
		R1 = "{base}/{TIME}/FQ/{sample}_R1.fastq.gz",
		R2 = "{base}/{TIME}/FQ/{sample}_R2.fastq.gz"
	version: config["bbtools"]
	params:
		rulename = "bbsplit",
		batch    = config[config['host']]["job_bbsplit"],
		source   = lambda wildcards: config["source"][wildcards.sample],
		work_dir =  WORK_DIR
	shell: """
	#######################
	if [ -d {input[0]} ]; then
		echo "This is a directory"
		name=`basename {input[0]}`
		obj_get -v MoCha ${{name}}_R1.fastq.gz --directory ${{LOCAL}}/
		obj_get -v MoCha ${{name}}_R2.fastq.gz --directory ${{LOCAL}}/
		
		if [ {params.source} == 'PDX' ]; then
			module load bbtools/{version}
			bbtools bbsplit build=1 -Xmx${{MEM}}g path={input.ref} in1=${{LOCAL}}/${{name}}_R1.fastq.gz in2=${{LOCAL}}/${{name}}_R2.fastq.gz basename={wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_%_#.fastq.gz refstats={wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_stat.txt
			mv -f {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_hg19_1.fastq.gz {output.R1} 
			mv -f {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_hg19_2.fastq.gz {output.R2}
			rm -rf {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_mm10_?.fastq.gz
		else
			cp ${{LOCAL}}/${{name}}_R1.fastq.gz {params.work_dir}/{output.R1}
			cp ${{LOCAL}}/${{name}}_R1.fastq.gz {params.work_dir}/{output.R2}
		fi
	else
		echo "Files are already here"
		R2=`echo "{input[0]}" |sed -e 's/_R1/_R2/g'`
		if [ {params.source} == 'PDX' ]; then
			module load bbtools/{version}
			bbtools bbsplit build=1 -Xmx${{MEM}}g path={input.ref} in1={input[0]} in2=${{R2}} basename={wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_%_#.fastq.gz refstats={wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_stat.txt
			mv -f {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_hg19_1.fastq.gz {output.R1}
			mv -f {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_hg19_2.fastq.gz {output.R2}
			rm -rf {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_mm10_?.fastq.gz
		else
			cp {input[0]} {params.work_dir}/{output.R1}
			cp ${{R2}}    {params.work_dir}/{output.R2}
		fi	
	fi		
	#######################
	"""
############
#	FASTQC
############
rule FASTQC:
	input:
		R1 = "{base}/{TIME}/FQ/{sample}_R1.fastq.gz",
		R2 = "{base}/{TIME}/FQ/{sample}_R2.fastq.gz",
	output:
		"{base}/{TIME}/{sample}/qc/fastqc/{sample}_R1_fastqc.html",
		"{base}/{TIME}/{sample}/qc/fastqc/{sample}_R2_fastqc.html"
	version: config["fastqc"]
	params:
		rulename  = "fastqc",
		batch     = config[config['host']]["job_fastqc"]
	shell: """
	#######################
	module load fastqc/{version}
	
	echo "Working on {input.R1}"
	zcat {input.R1} |fastqc --extract -t ${{THREADS}} -o {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/ -d ${{LOCAL}} /dev/stdin
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc.html {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R1_fastqc.html
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc      {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R1_fastqc
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc.zip  {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R1_fastqc.zip
	echo "Working on {input.R2}"
	zcat {input.R2} |fastqc --extract -t ${{THREADS}} -o {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/ -d ${{LOCAL}} /dev/stdin
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc.html {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R2_fastqc.html
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc      {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R2_fastqc
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc.zip  {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R2_fastqc.zip
	#######################
	"""
############
#       BWA
############
rule BWA:
	input:
		R1 = "{base}/{TIME}/FQ/{sample}_R1.fastq.gz",
		R2 = "{base}/{TIME}/FQ/{sample}_R2.fastq.gz",
		ref=config["bwaIndex"]
	output:
		temp("{base}/{TIME}/{sample}/{sample}.bwa.bam"),
		temp("{base}/{TIME}/{sample}/{sample}.bwa.bam.bai")
	version: config["bwa"]
	params:
		rulename  = "bwa",
		platform  = config["platform"],
		samtools  = config["samtools"],
		sentieon  = config["sentieon"],
		batch     = config[config['host']]["job_bwa"]
	shell: """
	#######################
	module load bwa/{version}
	module load samtools/{params.samtools}
	module load {params.sentieon}
	echo "Using Sentieon"
	bwa mem -M -t ${{THREADS}} -R '@RG\\tID:{wildcards.sample}\\tSM:{wildcards.sample}\\tLB:{wildcards.sample}\\tPL:{params.platform}' {input.ref} {input.R1} {input.R2} | sentieon util sort -o ${{LOCAL}}/{wildcards.sample}.bwa.bam -t ${{THREADS}} --sam2bam -i -
	cp ${{LOCAL}}/{wildcards.sample}.bwa.bam* {wildcards.base}/{TIME}/{wildcards.sample}/	
	#######################
	"""
############
#       GenotypeFile
############
# Using Older version of samtools for this purpose
rule GENOTYPING:
	input:
		bam="{base}/{TIME}/{sample}/{sample}.{aligner}.bam",
		interval=config["genotypeBed"],
		ref=config["reference"],
		vcf2genotype=NGS_PIPELINE + "/scripts/vcf2genotype.pl",
		vcf2loh=NGS_PIPELINE + "/scripts/vcf2loh.pl",
	output:
		vcf="{base}/{TIME}/{sample}/calls/{sample}.{aligner}.samtools.vcf",
		gt= "{base}/{TIME}/{sample}/qc/{sample}.{aligner}.gt",
		loh="{base}/{TIME}/{sample}/qc/{sample}.{aligner}.loh"
	version: config["samtools"]
	params:
		rulename  = "genotype",
		batch     = config[config['host']]["job_genotype"],
		dest	  = config["genotypeDest"],
		host	  = config["host"]
	shell: """
	#######################
	module load samtools/{version}
	samtools mpileup -u -C50 -f {input.ref} -l {input.interval} {input.bam} | bcftools view -gc - >{output.vcf}
	perl {input.vcf2genotype} {output.vcf} >{output.gt}
	
	perl {input.vcf2loh} {output.vcf} >{output.loh}
	
	#######################
	"""
############
# Gender Estimation
# loh2gender=NGS_PIPELINE + "/scripts/guessGender.pl",
############
rule Gender:
	input:
		loh="{base}/{TIME}/{sample}/qc/{sample}.bwa.loh",
		loh2gender=NGS_PIPELINE + "/scripts/guessGender.pl",
	output:
		"{base}/{TIME}/{sample}/qc/{sample}.gender"
	params:
		rulename  = "gender",
		batch     = config[config['host']]["job_default"],
	shell: """
	#######################
	perl {input.loh2gender} {input.loh} >{output}
	#######################
	"""
############
# Genotyping On Sample
############
rule SampleGT:
	input:
		gtFiles=lambda wildcards: SUB_GT[wildcards.subject],
		score=NGS_PIPELINE + "/scripts/scoreGenotyes.pl"
	output:
		"{subject}/{TIME}/qc/{subject}.genotyping.txt",
	params:
		rulename 	= "SampleGT",
		batch    	= config[config['host']]["job_default"],
		mail     	= config["mail"],
		mail_tool 	= NGS_PIPELINE + "/scripts/tsv2html.sh",
		host	 	= config["host"],
		diagnosis	= lambda wildcards: config['diagnosis'][wildcards.subject]
	shell: """
	#######################
	mkdir -p {wildcards.subject}/{TIME}/qc/GT
	mkdir -p {wildcards.subject}/{TIME}/qc/RATIO/
	cp {input.gtFiles} {wildcards.subject}/{TIME}/qc/GT/
	echo Sample >{wildcards.subject}/{TIME}/qc/RATIO/FirstColumn

	for file in {wildcards.subject}/{TIME}/qc/GT/*
	do
		sample=`basename ${{file}} .gt`
		echo ${{sample}} >>{wildcards.subject}/{TIME}/qc/RATIO/FirstColumn
		echo ${{sample}} >>{wildcards.subject}/{TIME}/qc/RATIO/${{sample}}.ratio
		for file2 in {wildcards.subject}/{TIME}/qc/GT/*
		do
			perl {input.score} ${{file}} ${{file2}} >>{wildcards.subject}/{TIME}/qc/RATIO/${{sample}}.ratio
		done
	done
	paste {wildcards.subject}/{TIME}/qc/RATIO/FirstColumn {wildcards.subject}/{TIME}/qc/RATIO/*.ratio >{wildcards.subject}/{TIME}/qc/{wildcards.subject}.genotyping.txt
	rm -rf {wildcards.subject}/{TIME}/qc/GT/ {wildcards.subject}/{TIME}/qc/RATIO/
	sed -i 's/Sample_//g' {wildcards.subject}/{TIME}/qc/{wildcards.subject}.genotyping.txt
	sed -i 's/.bwa//g' {wildcards.subject}/{TIME}/qc/{wildcards.subject}.genotyping.txt
	sed -i 's/.star//g' {wildcards.subject}/{TIME}/qc/{wildcards.subject}.genotyping.txt
	ssh -q {params.host} "sh {params.mail_tool} --name {wildcards.subject} --diagnosis '{params.diagnosis}' --head {WORK_DIR}/{wildcards.subject}/{TIME}/qc/{wildcards.subject}.genotyping.txt | mutt -e \\\"my_hdr Content-Type: text/html\\\" -s 'Genotyping Result on {wildcards.subject}' `whoami`@mail.nih.gov {params.mail} "
	#######################
	"""
############
#       QC_Sum
############
rule QC_Sum:
	input:
		TARGET,
		convertor = NGS_PIPELINE + "/scripts/makeQC.pl"
	output:
		"QC_AllSamples.txt"
	version: "v1.1"
	params:
		rulename = "qc_sum",
		batch    = config[config['host']]['job_default']
	shell: """
	#######################
	perl {input.convertor} `pwd` >{output}
	#######################
	"""
############
#       Samtools flagstat
############
rule FLAGSTAT:
	input:	"{base}/{TIME}/{sample}/{sample}.{aligner}.final.bam"
	output: "{base}/{TIME}/{sample}/qc/{sample}.{aligner}.flagstat.txt"
	version: config["samtools"]
	params:
		rulename  = "flagstat",
		batch     = config[config['host']]["job_flagstat"]
	shell: """
	#######################
	module load samtools/{version}
	samtools flagstat {input} > {output}
	#######################
	"""
############
#       Hotspot Coverage
############
rule HotSpotCoverage:
	input:
		bam="{base}/{TIME}/{sample}/{sample}.{aligner}.final.bam",
		interval=lambda wildcards: config['hotspot_bed'][config['sample_captures'][wildcards.sample]],
		genome=config["reference"].replace(".fasta",".genome")
	output: "{base}/{TIME}/{sample}/qc/{sample}.{aligner}.hotspot.depth"
	version: config["bedtools"]
	params:
		rulename  = "HotSpotCov",
		samtools  = config['samtools'],
		batch     = config[config['host']]["job_hotspot"],
	shell: """
	#######################
	module load samtools/{params.samtools}
	module load bedtools/2.22.0
	slopBed -i {input.interval} -g {input.genome} -b 50 >${{LOCAL}}/Region.bed
	samtools view -hF 0x400 -q 30 -L ${{LOCAL}}/Region.bed {input.bam} | samtools view -ShF 0x4 - | samtools view -SuF 0x200 - | bedtools coverage -abam - -b {input.interval} >{output}
	#######################
	"""
############
# Coverage
############
rule Coverage:
	input:
		bam="{subject}/{TIME}/{sample}/{sample}.bwa.final.bam",
		bai="{subject}/{TIME}/{sample}/{sample}.bwa.final.bam.bai",
		interval= lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample]],
	output:
		"{subject}/{TIME}/{sample}/qc/{sample}.bwa.coverage.txt"
	version: config["bedtools"]
	params:
		rulename = "coverage",
		batch    = config[config['host']]["job_bedtools"]
	shell: """
	#######################
	module load bedtools/{version}
	bedtools coverage -abam {input.bam} -b {input.interval} -hist |grep "^all" > {output}
	#######################
	"""
############
# IGV Session file
############
rule IGV_Session:
	input: bams=lambda wildcards: SUB_IGV[wildcards.subject]
	output: "{subject}/{TIME}/igv/session_{subject}.xml"
	message: "Making IGV session xml file for {wildcards.subject}"
	params:
		rulename = "igv_session",
		batch    = config[config['host']]["job_covplot"],
		work_dir =  WORK_DIR
	shell: """
	#######################
	dir=`echo {params.work_dir} | sed -e 's/\/data\/MoCha/M:/g' `
	echo "<?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?>" >{output}
	echo "<Global genome=\\"hg19\\" locus=\\"\\" version=\\"3\\">" >>{output}
	echo "\t<Resources>" >>{output}
	for BAM in {input.bams}
	do
		bam=`echo "${{dir}}/${{BAM}}" |sed -e 's/\//\\\\\\/g'`
		echo "\t\t<Resource path=\\"${{bam}}\\"/>" >>{output}
	done
	echo "\t</Resources>" >>{output}
	echo "</Global>" >>{output}

	########
	dir=`echo {params.work_dir} | sed -e 's/data/Volumes/g' `
	echo "<?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?>" >{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml
        echo "<Global genome=\\"hg19\\" locus=\\"\\" version=\\"3\\">" >>{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml
        echo "\t<Resources>" >>{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml
        for BAM in {input.bams}
        do
                echo "\t\t<Resource path=\\"${{dir}}/${{BAM}}\\"/>" >>{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml
        done
        echo "\t</Resources>" >>{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml
        echo "</Global>" >>{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml	
	#######################
	"""
############
#       Picard Mark Duplicates
############
rule Picard_MarkDup:
	input:
		bam="{subject}/{TIME}/{sample}/{sample}.{base}.bam",
		bai="{subject}/{TIME}/{sample}/{sample}.{base}.bam.bai"

	output:
		bam=temp("{subject}/{TIME}/{sample}/{sample}.{base}.dd.bam"),
		index=temp("{subject}/{TIME}/{sample}/{sample}.{base}.dd.bam.bai"),
		metrics="{subject}/{TIME}/{sample}/qc/{base}.markdup.txt"
	version: config["picard"]
	params:
		rulename  = "mark_dup",
		batch     = config[config['host']]["job_markdup"],
		samtools  = config["samtools"],
		sentieon  = config["sentieon"],
	shell: """
	#######################
	module load picard/{version}
	module load {params.sentieon}
	echo "Using Sentieon"
	sentieon driver -t ${{THREADS}} -i {input.bam} --algo LocusCollector --fun score_info ${{LOCAL}}/{wildcards.base}.markdup.txt.tmp
	sentieon driver -t ${{THREADS}} -i {input.bam} --algo Dedup --score_info ${{LOCAL}}/{wildcards.base}.markdup.txt.tmp --metrics ${{LOCAL}}/{wildcards.base}.markdup.txt ${{LOCAL}}/{wildcards.sample}.{wildcards.base}.dd.bam
	cp ${{LOCAL}}/{wildcards.base}.markdup.txt {output.metrics}
	cp ${{LOCAL}}/{wildcards.sample}.{wildcards.base}.dd.bam {output.bam}
	cp ${{LOCAL}}/{wildcards.sample}.{wildcards.base}.dd.bam.bai {output.index}
	#######################
	"""
############
#       GATK Best Practices
############
rule GATK:
	input: 	bam="{base}/{TIME}/{sample}/{sample}.bwa.dd.bam",
		bai="{base}/{TIME}/{sample}/{sample}.bwa.dd.bam.bai",
		ref=config["reference"],
		phase1=config["1000G_phase1"],
		mills=config["Mills_and_1000G"]
	output:
		bam="{base}/{TIME}/{sample}/{sample}.bwa.final.bam",
		index="{base}/{TIME}/{sample}/{sample}.bwa.final.bam.bai",
	params:
		rulename  = "gatk",
		batch     = config[config['host']]["job_gatk"],
		sentieon  = config["sentieon"],
	shell: """
	#######################
	module load {params.sentieon}
	echo "Using Sentieon"
	sentieon driver --temp_dir ${{LOCAL}}/ -t ${{THREADS}} -r {input.ref} -i {input.bam} --algo Realigner -k {input.phase1} -k {input.mills} ${{LOCAL}}/{wildcards.sample}.lr.bam
	sentieon driver --temp_dir ${{LOCAL}}/ -t ${{THREADS}} -r {input.ref} -i ${{LOCAL}}/{wildcards.sample}.lr.bam --algo QualCal -k {input.phase1} -k {input.mills} ${{LOCAL}}/{wildcards.sample}.recalibration.matrix.txt
	sentieon driver --temp_dir ${{LOCAL}}/ -t ${{THREADS}} -r {input.ref} -i ${{LOCAL}}/{wildcards.sample}.lr.bam -q ${{LOCAL}}/{wildcards.sample}.recalibration.matrix.txt --algo QualCal -k {input.phase1} -k {input.mills} ${{LOCAL}}/{wildcards.sample}.recalibrationPost.matrix.txt --algo ReadWriter ${{LOCAL}}/{wildcards.sample}.bwa.final.bam
	cp ${{LOCAL}}/{wildcards.sample}.bwa.final.bam {output.bam}
	cp ${{LOCAL}}/{wildcards.sample}.bwa.final.bam.bai {output.index}

	sleep 60s
	touch {output.index}
	#######################
	"""
############
#       MuTect
############
rule MuTect:
	input:
		lambda wildcards: somaticPairs[wildcards.Tumor],
		ref=config["reference"],
		dbsnp=config["dbsnp"],
		cosmic=config["cosmic"],
		interval=lambda wildcards: config['target_intervals'][pairedCapture[wildcards.Tumor]].replace("target","targetbp")
	output:
		vcf="{subject}/{TIME}/{Tumor}/calls/{Tumor}.MuTect.raw.vcf",
		call_stats="{subject}/{TIME}/{Tumor}/qc/{Tumor}.mutect.call_stats.txt",
		coverage="{subject}/{TIME}/{Tumor}/qc/{Tumor}.mutect.coverage.wig.txt"
	version: config["MuTect"]
	params:
		rulename = "MuTect",
		R	 = config['version_R'],
		batch    = config[config['host']]["job_mutect"],
		normal   = lambda wildcards: config['matched_normal'][wildcards.Tumor][0],
		vcforder = NGS_PIPELINE + "/scripts/vcfOrderCol.R",
		sentieon =config["sentieon"],
		mt_sen   ="--max_normal_alt_frac 0.05 --max_normal_alt_cnt 4 --min_base_qual 20 ",
	shell: """
	#######################
	module load {params.sentieon}
	echo "Using Sentieon"
	sentieon driver --temp_dir ${{LOCAL}}/\
		--read_filter MapQualFilter,min_map_qual=30 -t ${{THREADS}} -r {input.ref} \
		-i {input[2]} -i {input[0]} \
		--interval {input.interval} \
		--algo TNsnv --dbsnp {input.dbsnp} \
		--tumor_sample {wildcards.Tumor} --normal_sample {params.normal} \
		--call_stats_out {output.call_stats} --stdcov_out {output.coverage} {params.mt_sen} ${{LOCAL}}/{wildcards.Tumor}.vcf
	{params.vcforder} -i ${{LOCAL}}/{wildcards.Tumor}.vcf -o {output.vcf}
	#######################
	"""
############
#       MuTect2
############
rule MuTect2:
	input:
		lambda wildcards: somaticPairs[wildcards.Tumor],
		ref=config["reference"],
		dbsnp=config["dbsnp"],
		cosmic=config["cosmic"],
		Twobit=config["reference"].replace('.fasta', '.2bit'),
		interval=lambda wildcards: config['target_intervals'][pairedCapture[wildcards.Tumor]].replace("target","targetbp")
	output:
		vcf="{subject}/{TIME}/{Tumor}/calls/{Tumor}.MuTect2.raw.vcf",
	params:
		rulename = "MuTect2",
		batch    = config[config['host']]["job_mutect"],
		normal   = lambda wildcards: config['matched_normal'][wildcards.Tumor][0],
		vcforder = NGS_PIPELINE + "/scripts/vcfOrderCol.R",
		sentieon =config["sentieon"],
		script2=NGS_PIPELINE+"/scripts/joinAdjacentSNPs.py",
		mt_sen   ="--max_normal_alt_frac 0.05 --max_normal_alt_cnt 4 --min_base_qual 20 ",
	shell: """
	#######################
	module load {params.sentieon}
	sentieon driver --temp_dir ${{LOCAL}}/\
		--read_filter MapQualFilter,min_map_qual=30 -t ${{THREADS}} -r {input.ref} \
		-i {input[2]} -i {input[0]} \
		--interval {input.interval} \
		--algo TNhaplotyper --dbsnp {input.dbsnp} \
		--tumor_sample {wildcards.Tumor} --normal_sample {params.normal} ${{LOCAL}}/{wildcards.Tumor}.vcf
	{params.vcforder} -i ${{LOCAL}}/{wildcards.Tumor}.vcf -o ${{LOCAL}}/{wildcards.Tumor}.1.vcf
	module load samtools
	python {params.script2} -v ${{LOCAL}}/{wildcards.Tumor}.1.vcf -o {output.vcf} 1 {input[2]} {input.Twobit}	
	#######################
	"""
############
#	vcf2MAF on MuTect output
############
rule muTect2maf:
	input:
		vcf="{subject}/{TIME}/{sample}/calls/{sample}.{caller}.raw.vcf",
		ref=config["reference"]
	output:
		"{subject}/{TIME}/{sample}/calls/{sample}.{caller}.maf",
	version: config["vcf2maf"]
	params:
		rulename = "vcf2maf",
		isoforms = config['mskcc_isoforms'],
		batch    = config[config['host']]["job_HC_PLAT"],
		VEP      = config['VEP'],
		normal   = lambda wildcards: config['matched_normal'][wildcards.sample][0],
	shell: """
	#######################
	module load vcf2maf/{version} samtools VEP/{params.VEP}
	grep -P "#" {input.vcf} >${{LOCAL}}/input.vcf
	grep -P "\\tPASS\\t" {input.vcf} >>${{LOCAL}}/input.vcf
        vcf2maf.pl --input-vcf ${{LOCAL}}/input.vcf --output-maf ${{LOCAL}}/{wildcards.sample}.{wildcards.caller}.maf --tumor-id {wildcards.sample} --normal-id {params.normal} --ref-fasta {input.ref} --filter-vcf /fdb/VEP/89/cache/ExAC.r0.3.1.sites.vep.vcf.gz --vep-path  $VEPHOME --vep-forks ${{THREADS}} --vep-data $VEP_CACHEDIR --custom-enst {params.isoforms}
        cp ${{LOCAL}}/{wildcards.sample}.{wildcards.caller}.maf {wildcards.subject}/{TIME}/{wildcards.sample}/calls/{wildcards.sample}.{wildcards.caller}.maf
	#######################
	"""
############
#       Strelka
############
rule Strelka:
	input:
		lambda wildcards: somaticPairs[wildcards.Tumor],
		ref=config["reference"],
		interval=lambda wildcards: config['target_intervals'][pairedCapture[wildcards.Tumor]].replace("target","targetbp")
	output:
		snps="{subject}/{TIME}/{Tumor}/calls/{Tumor}.strelka.snvs.raw.vcf",
		indels="{subject}/{TIME}/{Tumor}/calls/{Tumor}.strelka.indels.raw.vcf"
	version: config["strelka"]
	params:
		rulename = "Strelka",
		batch    = config[config['host']]["job_strelka"],
		config=NGS_PIPELINE + "/Tools_config/"+config["strelka_config"],
		vcftools = config["vcftools"]
	shell: """
	#######################
	module load strelka/{version}
	configureStrelkaWorkflow.pl --normal={input[2]} --tumor={input[0]}\
	--ref={input.ref} --config={params.config} --output-dir=${{LOCAL}}/strelka
	make -j ${{SLURM_CPUS_PER_TASK}} -f ${{LOCAL}}/strelka/Makefile
	module load vcftools/{params.vcftools}
	vcftools --vcf ${{LOCAL}}/strelka/results/passed.somatic.snvs.vcf --bed {input.interval} --out {output.snps} --recode --keep-INFO-all
	mv -f {output.snps}.recode.vcf {output.snps}
	vcftools --vcf ${{LOCAL}}/strelka/results/passed.somatic.indels.vcf --bed {input.interval}  --out {output.indels} --recode --keep-INFO-all
	mv -f {output.indels}.recode.vcf {output.indels}
	NORMAL=`basename {input[2]} .bwa.final.bam`
	sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t{wildcards.Tumor}/g" {output.snps}
	sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t{wildcards.Tumor}/g" {output.indels}

	#######################
	"""
############
#	snpEff
############
rule SNPEff:
	input:
		vcf="{subject}/{TIME}/{sample}/calls/{base}.raw.vcf",
		ref=config["reference"],
	output:
		eff="{subject}/{TIME}/{sample}/calls/{base}.raw.snpEff.vcf"
	version: config["snpEff"]
	params:
		rulename      ="snpEff",
		batch	      =config[config['host']]["job_snpeff"],
		snpEff_genome =config["snpEff_genome"],
		snpEff_config=NGS_PIPELINE + "/Tools_config/"+config["snpEff_config"],
		annovar       =config["annovar"]
	shell: """
	#######################
	module load snpEff/{version}
	java -Xmx${{MEM}}g -Djava.io.tmpdir=${{LOCAL}} -jar $SNPEFF_JARPATH/SnpSift.jar dbnsfp -c {params.snpEff_config} -a {input.vcf} | java -Xmx${{MEM}}g -jar $SNPEFF_JARPATH/snpEff.jar -t -canon {params.snpEff_genome} > {output.eff}
	#######################
	"""
############
#       vcf2txt
############
rule VCF2TXT:
	input:
		eff="{subject}/{TIME}/{sample}/calls/{base}.raw.snpEff.vcf",
		vcf2txt=NGS_PIPELINE + "/scripts/vcf2txt.pl"
	output:
		txt="{subject}/{TIME}/{sample}/calls/{base}.snpEff.txt"
	params:
		rulename      ="vcf2txt",
		batch         =config[config['host']]["job_default"],
		annovar       =config["annovar"]
	shell: """
	#######################
	module load annovar/{params.annovar}
	perl {input.vcf2txt} {input.eff} ${{LOCAL}} >{output.txt}
	#######################
	"""
############
#	MakeList
############
rule FormatInput:
	input:
		txtFiles=lambda wildcards: SUBJECT_VCFS[wildcards.subject],
		#hotspot ="{subject}/{TIME}/{subject}/db/{subject}.hotspot",
		convertor= NGS_PIPELINE + "/scripts/MakeAnnotationInputs.pl"
	output:
		temp("{subject}/{TIME}/annotation/AnnotationInput.anno"),
		temp("{subject}/{TIME}/annotation/AnnotationInput.sift")
	version: config["annovar"]
	params:
		rulename   = "FormatInput",
		batch      = config[config['host']]["job_default"],
		fAEV       = NGS_PIPELINE + "/scripts/findAlreadyExistingVariants.pl"
	shell: """
	#######################
	module load annovar/{version}
	export LC_ALL=C
	cut -f 1-5 {input.txtFiles} |sort |uniq > {wildcards.subject}/{TIME}/annotation/AnnotationInput
	perl {input.convertor} {wildcards.subject}/{TIME}/annotation/AnnotationInput
	rm -rf "{wildcards.subject}/{TIME}/annotation/AnnotationInput.pph"
	#######################
	"""
############
#       CopyAnnotationFile
############
rule CopyAnnotationFile:
	input:
		"{subject}/{TIME}/annotation/AnnotationInput.coding.rare.txt"
	output:
		"{subject}/{TIME}/annotation/{subject}.Annotations.coding.rare.txt"
	params:
		rulename   = "caf",
		batch      = config[config['host']]["job_default"],
	shell: """
	#######################
	cp {input} {output}
	#######################
	"""
############
#	Add Annotation back to sample level file
############
rule AttachAnnotation:
	input:
		txt="{subject}/{TIME}/{base1}/calls/{base}.snpEff.txt",
		ref="{subject}/{TIME}/annotation/{subject}.Annotations.coding.rare.txt",
		convertor  = NGS_PIPELINE + "/scripts/addAnnotations2vcf.pl"
	output:
		txt="{subject}/{TIME}/{base1}/calls/{base}.annotated.txt"
	version: "1.0"
	params:
		rulename   = "add",
		batch      = config[config['host']]["job_addbackann"],
	shell: """
	#######################
	perl {input.convertor} {input.ref}  {input.txt} >{output.txt}
	#######################
	"""
############
#       Expressed
############
rule Expressed:
	input:
		RNASeq = lambda wildcards: expressedPairs[wildcards.sample],
		Mutation="{subject}/{TIME}/{sample}/calls/{base}.annotated.txt",
		convertor = NGS_PIPELINE + "/scripts/mpileup.pl"
	output: "{subject}/{TIME}/{sample}/calls/{base}.annotated.expressed.txt"
	version: config["samtools"]
	params:
		rulename  = "Expressed",
		batch     = config[config['host']]["job_expressed"],
		name      = lambda wildcards: config["matched_rnaseq"][wildcards.sample]
	shell: """
	#######################
	module load samtools/{version}
	perl {input.convertor} {input.Mutation} {wildcards.subject}/{TIME}/{params.name}/{params.name}.star.final.bam {input.RNASeq} >{output}	
	#######################
	"""
############
#       Database Input
############
rule DBinput:
	input:
		txtFiles=lambda wildcards: SUBJECT_ANNO[wildcards.subject][wildcards.group],
		convertor=NGS_PIPELINE + "/scripts/makeDBVariantFile.pl",
		tool=NGS_PIPELINE + "/scripts/AddSampleType.pl",
		tool1=NGS_PIPELINE + "/scripts/addFS.pl",
		txtFiles1=lambda wildcards: SUBJECT_VCFS[wildcards.subject]
	output: "{subject}/{TIME}/{subject}/db/{subject}.{group}"
	params:
		rulename = "makeDBinput",
		batch    = config[config['host']]['job_default'],
		hash 	 = lambda wc: " ".join("{} {}".format(a, b) for a, b in config["sample_type"].items()),
		hash1	 = lambda wc: " ".join("{} {}".format(a, b) for a, b in config["sample_captures"].items()),
	shell: """
	#######################
	perl {input.convertor} {input.txtFiles} |perl {input.tool} - "{params.hash}" "{params.hash1}" >{output}.tmp
	if [ {wildcards.group}  == 'germline' ] || [ {wildcards.group} == 'variants' ]; then	
		perl {input.tool1} {output}.tmp {input.txtFiles1} >{output}
		rm -rf {output}.tmp
	else
		mv {output}.tmp {output}
	fi
	#######################
	"""
