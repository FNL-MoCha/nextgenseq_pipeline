import itertools
import os
import collections
import json
import glob
from snakemake.utils import R
from snakemake.utils import min_version
min_version("3.2")
from snakemake.exceptions import MissingInputException
# Snakemake Base location
NGS_PIPELINE=os.environ['NGS_PIPELINE']
WORK_DIR=os.environ['WORK_DIR']
DATA_DIR=os.environ['DATA_DIR']
ACT_DIR=os.environ['ACT_DIR']
HOST=os.environ['HOST']
TIME=os.environ['TIME']
NOW=os.environ['TMP']
configfile: NGS_PIPELINE +"/config/config_annotation.json"
configfile: NGS_PIPELINE +"/config/config_common.json"
configfile: NGS_PIPELINE +"/config/config_cluster.json"
if HOST == 'biowulf.nih.gov':
	configfile: NGS_PIPELINE +"/config/config_common_biowulf.json"
elif HOST == 'login01':
	configfile: NGS_PIPELINE +"/config/config_common_tgen.json"
elif HOST == 'moab':
	configfile: NGS_PIPELINE +"/config/config_common_moab.json"

config['host'] = HOST
GROUP=config['group']
VERSION=config["pipeline_version"]
MAIL=config['mail']
#HOST = config['host']
###########################################################################
#
#		This initializes all the variables we need for the jobs.
#		It also removes the host specific constraints like scratch
#		area on the node.
###########################################################################
shell.prefix("""
set -e -o pipefail
module use --prepend /data/MoCha/patidarr/modules
sleep 20s
MEM=`echo "${{SLURM_MEM_PER_NODE}} / 1024 "|bc`
LOCAL="/lscratch/${{SLURM_JOBID}}/"
THREADS=${{SLURM_CPUS_ON_NODE}}
""")
###########################################################################
#
#			Conversion
#
###########################################################################
SUBJECT_TO_SAMPLE  = {}
for subject in config['DNASeq']:
	SUBJECT_TO_SAMPLE[subject] = expand("{sample}", sample = config['DNASeq'][subject])
###########################################################################
SAMPLE_TO_SUBJECT  = {}
for subject,samples in config['DNASeq'].items():
	for sample in samples:
		SAMPLE_TO_SUBJECT[sample]=subject
###########################################################################
#make dictionary containing fastq file locations.
# Die if a library is ran twice or more.
FQ={}
for sample in config['library'].keys():
	for fq in config['library'][sample]:
		if len(config['library'][sample]) == 1:
			if os.path.isfile(DATA_DIR+fq+"/"+fq+"_R1.fastq.gz"):
				FQ[sample] =[DATA_DIR+fq+"/"+fq+"_R1.fastq.gz", DATA_DIR+fq+"/"+fq+"_R2.fastq.gz"]
			elif os.path.isfile(DATA_DIR+"Sample_"+fq+"/Sample_"+fq+"_R1.fastq.gz"):
				FQ[sample] =[DATA_DIR+"Sample_"+fq+"/Sample_"+fq+"_R1.fastq.gz", DATA_DIR+"Sample_"+fq+"/Sample_"+fq+"_R2.fastq.gz"]
			elif os.path.exists(DATA_DIR+fq):
				R1=glob.glob(DATA_DIR+fq)
				FQ[sample] =[]
				FQ[sample].extend(R1)
			elif os.path.exists(DATA_DIR+"Sample_"+fq):
				R1=glob.glob(DATA_DIR+"Sample_"+fq)
				FQ[sample] =[]
				FQ[sample].extend(R1)
			else:
				print("#####################################")
				print("")
				print("")
				print("Can not find fastq files for", sample)
				print("")
				print("")
				print("#####################################")
				exit()
		else:
			print("#####################################")
			print("")
                        print("")
			print("Can not process fastq files from two locations for ",sample )
			print("")
			print("")
			print("#####################################")
			exit()
###########################################################################
####
#### Targets
####
PATIENTS =[]
SUB_BAMS= {}
SUB_COV = {}
SUB_LOH = {}
SUB_GT  = {}
SUB_HOT = {}
SAMPLES =[]
somaticPairs = {}
pairedCapture = {}
# Inputs for the targets, where direct list can not be used.
for subject in config['DNASeq'].keys():
	PATIENTS.append(subject)
	SUB_BAMS[subject]= ["{subject}/{TIME}/{sample}/{sample}.bwa.final.bam".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['DNASeq'][subject]]
	SUB_COV[subject] = ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.coverage.txt".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['DNASeq'][subject]]
	SUB_HOT[subject] = ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.hotspot.depth".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['DNASeq'][subject]]
	SUB_LOH[subject] = ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.loh".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['DNASeq'][subject]]
	SUB_GT[subject]  = ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.gt".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['DNASeq'][subject]]
	for sample in config['DNASeq'][subject]:
		SAMPLES.append(sample)
###########################################################################
#		Add RNASeq only samples to PATIENTS
###########################################################################
for subject in config['RNASeq']:
	SUBJECT_TO_SAMPLE[subject] = expand("{sample}", sample = config['RNASeq'][subject])
for subject  in config['RNASeq'].keys():
        if subject not in PATIENTS:
                PATIENTS.append(subject)
###########################################################################
# Many of the targets.
TARGET      = ["{subject}/{TIME}/{sample}/qc/fastqc/{sample}_R2_fastqc.html".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.flagstat.txt".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += ["{subject}/{TIME}/{sample}/{sample}.bwa.final.bam".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.hotspot.depth".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += ["{subject}/{TIME}/{sample}/qc/{sample}.bwa.gt".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += ["{subject}/{TIME}/{sample}/qc/{sample}.gender".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
TARGET     += expand("{subject}/{TIME}/qc/{subject}.genotyping.txt", TIME=TIME, subject=PATIENTS)

if len(config['matched_normal']) > 0:
	for Tumor in config['matched_normal']:
		for Normal in config['matched_normal'][Tumor]:
			TumorBam   = "{subject}/{TIME}/{sample}/{sample}.bwa.final".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)
			NormalBam  = "{subject}/{TIME}/{sample}/{sample}.bwa.final".format(TIME=TIME, subject=SAMPLE_TO_SUBJECT[Normal], sample=Normal)
			pairedCapture[Tumor] = config['sample_captures'][Tumor]
			somaticPairs[Tumor] = [TumorBam + ".bam" , TumorBam + ".bam.bai", NormalBam + ".bam", NormalBam + ".bam.bai"]
for sample in config['matched_normal'].keys():
	local  = []
	subject=SAMPLE_TO_SUBJECT[sample]
	TARGET += [subject+"/"+TIME+"/"+sample+"/calls/"+sample+".strelka.snvs.raw.vcf"],
	TARGET += [subject+"/"+TIME+"/"+sample+"/calls/"+sample+".strelka.indels.raw.vcf"],
	TARGET += [subject+"/"+TIME+"/"+sample+"/calls/"+sample+".MuTect.maf"],
	TARGET += [subject+"/"+TIME+"/"+sample+"/calls/"+sample+".MuTect2.maf"],
###########################################################################
localrules: Pipeline, RNASeq, makeConfig, QC_Summary_Patient,QC_Summary,TOPHAT_LINK, SampleGT,QC_Sum, RNASeqQC2,RNASeqQC3, Cuff_Mat
#Circos, CoveragePlot, BoxPlot_Hotspot, makeConfig,Ideogram
###########################################################################
#				Include Rule Book			  #
###########################################################################
include: NGS_PIPELINE +"/ruleBook/oncoKB.annotator.rules"
include: NGS_PIPELINE +"/ruleBook/MAGGIE.rules"
include: NGS_PIPELINE +"/ruleBook/ROH.rules"
include: NGS_PIPELINE +"/ruleBook/TMB.rules"
include: NGS_PIPELINE +"/ruleBook/variantsHeatmap.rules"
include: NGS_PIPELINE +"/ruleBook/MSI.rules"
include: NGS_PIPELINE +"/ruleBook/mixcr.rules"
include: NGS_PIPELINE +"/ruleBook/Ancestry.rules"
include: NGS_PIPELINE +"/ruleBook/mergeHC_PLAT.rules"
include: NGS_PIPELINE +"/ruleBook/verifyBamID.rules"
include: NGS_PIPELINE +"/ruleBook/rnaseq_pipeline.rules"
include: NGS_PIPELINE +"/ruleBook/rnaseqQC.rules"
include: NGS_PIPELINE +"/ruleBook/readDepth.rules"
include: NGS_PIPELINE +"/ruleBook/failedExon.rules"
include: NGS_PIPELINE +"/ruleBook/hsMetrix.rules"
include: NGS_PIPELINE +"/ruleBook/Consolidate.rules"
#include: NGS_PIPELINE +"/ruleBook/universal.rules"
include: NGS_PIPELINE +"/ruleBook/mutationalSignature.rules"
#include: NGS_PIPELINE +"/ruleBook/NeoAntigen.rules"
include: NGS_PIPELINE +"/ruleBook/haplotypeCaller.rules"
include: NGS_PIPELINE +"/ruleBook/platypus.rules"
include: NGS_PIPELINE +"/ruleBook/gatk_RNASeq.rules"
include: NGS_PIPELINE +"/ruleBook/ideogram.rules"
#include: NGS_PIPELINE +"/ruleBook/Actionable.rules"
#include: NGS_PIPELINE +"/ruleBook/UnionSomaticMutations.rules" # This one activates Strelka 
include: NGS_PIPELINE +"/ruleBook/plots.rules"
#include: NGS_PIPELINE +"/ruleBook/annot.rules"
include: NGS_PIPELINE +"/ruleBook/featureCount.rules"
include: NGS_PIPELINE +"/ruleBook/Sequenza.rules"
include: NGS_PIPELINE +"/ruleBook/cnvkit.rules"
#include: NGS_PIPELINE +"/ruleBook/MethlySeq.rules"
include: NGS_PIPELINE +"/ruleBook/RSEM.rules"
#include: NGS_PIPELINE +"/ruleBook/HotSpot_Pileup.rules"
#include: NGS_PIPELINE +"/ruleBook/PyClone.rules"
###########################################################################
# we have to do it this way as some samples may not have rna or tumor     #
###########################################################################
onerror:
	shell("find .snakemake/ ! -readable -prune \( -type f -user $USER -exec chmod g+rw {{}} \; \) , \( -type d -user $USER -exec chmod g+rwx {{}} \; \)")
        shell("find .snakemake/ ! -readable -prune -group $USER -exec chgrp -f {GROUP} {{}} \;")
	shell("find {PATIENTS} -group $USER -exec chgrp -f {GROUP} {{}} \;")
	shell("find {PATIENTS} \( -type f -user $USER -exec chmod g+rw {{}} \; \) , \( -type d -user $USER -exec chmod g+rwx {{}} \; \)")
	shell("ssh -q {HOST} \"echo 'ngs-pipeline version {VERSION} failed on {PATIENTS}. Error occured on {HOST}. Working Dir:  {WORK_DIR}' |mutt -s ' ngs-pipeline Status' `whoami`@mail.nih.gov -c patidarr@mail.nih.gov\"")
	shell("find .snakemake/ ! -readable -prune -group $USER -exec chgrp -f {GROUP} {{}} \;")
onstart:
	f = open('ngs_pipeline_%s.csv' % NOW , 'w')
	print ('#Patient','diagnosis','CaseID',sep='\t', end='\n',file=f)
	for subject in sorted(PATIENTS):
		diagnosis =config['diagnosis'][subject]
		print (subject,diagnosis,TIME,sep='\t', end='\n',file=f)
	
	shell("for sub in {PATIENTS}; do rm -rf {WORK_DIR}/${{sub}}/{TIME}/successful.txt ; done")
	shell("ssh -q {HOST} \"echo 'ngs-pipeline version {VERSION} started on {PATIENTS} on {HOST}. Working Dir:  {WORK_DIR}' |mutt -s ' ngs-pipeline Status' `whoami`@mail.nih.gov -c patidarr@mail.nih.gov\"")
onsuccess:
	shell("find .snakemake/ ! -readable -prune \( -type f -user $USER -exec chmod g+r {{}} \; \) , \( -type d -user $USER -exec chmod g+rwx {{}} \; \)")
	shell("find .snakemake/ ! -readable -prune -group $USER -exec chgrp -f {GROUP} {{}} \;")
	print("Workflow finished, no error")
###########################################################################
rule pipeline:
	input:
		TARGET,
		expand ("ngs_pipeline_{NOW}.rnaseq.done", NOW=NOW)
	version: config["pipeline_version"]
	wildcard_constraints:
		NOW="\w+"
	params:
		rulename = "pipeline",
		batch    = config[config['host']]["job_default"],
		group    = config["group"],
		mail 	 = NGS_PIPELINE + "/scripts/tsv2html.final.sh",
		email    = config["mail"],
		host     = config["host"],
		subs     = PATIENTS
	shell: """
	#######################
	rm -rf ngs_pipeline_{NOW}.rnaseq.done
	find {PATIENTS} log -group $USER -exec chgrp -f {params.group} {{}} \;
	find {PATIENTS} log \( -type f -user $USER -exec chmod g+rw {{}} \; \) , \( -type d -user $USER -exec chmod g+rwx {{}} \; \)
	export LC_ALL=C
	
	for sub in {params.subs}
        do
                touch {WORK_DIR}/${{sub}}/{TIME}/successful.txt
		chmod g+rw {WORK_DIR}/${{sub}}/{TIME}/successful.txt 	
		chgrp {params.group} {WORK_DIR}/${{sub}}/{TIME}/successful.txt
        done
	ssh -q {params.host} "{params.mail} --location {WORK_DIR} --host {params.host} --head --version {version} {WORK_DIR}/ngs_pipeline_{NOW}.csv |mutt -e \\\"my_hdr Content-Type: text/html\\\" -s ' ngs-pipeline Status' `whoami`@mail.nih.gov -c patidarr@mail.nih.gov"
	rm -rf {WORK_DIR}/ngs_pipeline_{NOW}.csv
	#######################
	"""
############
#       Merge Fastq Files
############
rule bbsplit:
	input: 
		R=lambda wildcards: FQ[wildcards.sample],
		ref=config['bbsplitIndex']
	output:
		R1 = "{base}/{TIME}/FQ/{sample}_R1.fastq.gz",
		R2 = "{base}/{TIME}/FQ/{sample}_R2.fastq.gz"
	version: config["bbtools"]
	params:
		rulename = "bbsplit",
		batch    = config[config['host']]["job_bbsplit"],
		subsample= NGS_PIPELINE+"/scripts/subsample.sh",
		library   =lambda wildcards: config['sample_type'][wildcards.sample],
		source   = lambda wildcards: config["source"][wildcards.sample],
		work_dir =  WORK_DIR
	shell: """
	#######################
	if [ -d {input[0]} ]; then
		echo "This is a directory"
		name=`basename {input[0]}`
		obj_get -v MoCha ${{name}}_R1.fastq.gz --directory ${{LOCAL}}/
		obj_get -v MoCha ${{name}}_R2.fastq.gz --directory ${{LOCAL}}/
		if [ {params.library} == 'RNASeq' ]; then
			sh {params.subsample} ${{LOCAL}}/${{name}}_R1.fastq.gz ${{LOCAL}}/${{name}}_R2.fastq.gz
		fi
		if [ {params.source} == 'PDX' ]; then
			module load bbtools/{version}
			bbtools bbsplit build=1 -Xmx${{MEM}}g path={input.ref} in1=${{LOCAL}}/${{name}}_R1.fastq.gz in2=${{LOCAL}}/${{name}}_R2.fastq.gz basename={wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_%_#.fastq.gz refstats={wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_stat.txt
			mv -f {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_hg19_1.fastq.gz {output.R1} 
			mv -f {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_hg19_2.fastq.gz {output.R2}
			rm -rf {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_mm10_?.fastq.gz
		else
			cp ${{LOCAL}}/${{name}}_R1.fastq.gz {params.work_dir}/{output.R1}
			cp ${{LOCAL}}/${{name}}_R2.fastq.gz {params.work_dir}/{output.R2}
		fi
	else
		echo "Files are still here"
		R2=`echo "{input[0]}" |sed -e 's/_R1.fastq.gz/_R2.fastq.gz/g'`
		if [ {params.library} == 'RNASeq' ]; then
			sh {params.subsample} {input[0]} ${{R2}}
		fi
		
		obj_put -v MoCha --force --progressbar {input[0]} ${{R2}}
		
		if [ {params.source} == 'PDX' ]; then
			module load bbtools/{version}
			bbtools bbsplit build=1 -Xmx${{MEM}}g path={input.ref} in1={input[0]} in2=${{R2}} basename={wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_%_#.fastq.gz refstats={wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_stat.txt
			mv -f {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_hg19_1.fastq.gz {output.R1}
			mv -f {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_hg19_2.fastq.gz {output.R2}
			rm -rf {wildcards.base}/{wildcards.TIME}/FQ/{wildcards.sample}_mm10_?.fastq.gz
		else
			cp {input[0]} {params.work_dir}/{output.R1}
			cp ${{R2}}    {params.work_dir}/{output.R2}
		fi	
		rm -rf {input[0]} ${{R2}}
		touch {params.work_dir}/{output.R1} {params.work_dir}/{output.R2}
	fi		
	#######################
	"""
############
#	FASTQC
############
rule fastqc:
	input:
		R1 = "{base}/{TIME}/FQ/{sample}_R1.fastq.gz",
		R2 = "{base}/{TIME}/FQ/{sample}_R2.fastq.gz",
	output:
		"{base}/{TIME}/{sample}/qc/fastqc/{sample}_R1_fastqc.html",
		"{base}/{TIME}/{sample}/qc/fastqc/{sample}_R2_fastqc.html"
	version: config["fastqc"]
	params:
		rulename  = "fastqc",
		batch     = config[config['host']]["job_fastqc"]
	shell: """
	#######################
	module load fastqc/{version}
	
	echo "Working on {input.R1}"
	zcat {input.R1} |fastqc --extract -t ${{THREADS}} -o {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/ -d ${{LOCAL}} /dev/stdin
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc.html {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R1_fastqc.html
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc      {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R1_fastqc
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc.zip  {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R1_fastqc.zip
	echo "Working on {input.R2}"
	zcat {input.R2} |fastqc --extract -t ${{THREADS}} -o {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/ -d ${{LOCAL}} /dev/stdin
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc.html {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R2_fastqc.html
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc      {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R2_fastqc
	mv -f {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/stdin_fastqc.zip  {wildcards.base}/{TIME}/{wildcards.sample}/qc/fastqc/{wildcards.sample}_R2_fastqc.zip
	#######################
	"""
############
#       BWA
############
rule bwa:
	input:
		R1 = "{base}/{TIME}/FQ/{sample}_R1.fastq.gz",
		R2 = "{base}/{TIME}/FQ/{sample}_R2.fastq.gz",
		ref=config["bwaIndex"]
	output:
		temp("{base}/{TIME}/{sample}/{sample}.bwa.bam"),
		temp("{base}/{TIME}/{sample}/{sample}.bwa.bam.bai")
	version: config["bwa"]
	params:
		rulename  = "bwa",
		platform  = config["platform"],
		samtools  = config["samtools"],
		sentieon  = config["sentieon"],
		batch     = config[config['host']]["job_bwa"]
	shell: """
	#######################
	module load bwa/{version}
	module load samtools/{params.samtools}
	module load {params.sentieon}
	echo "Using Sentieon"
	bwa mem -M -t ${{THREADS}} -R '@RG\\tID:{wildcards.sample}\\tSM:{wildcards.sample}\\tLB:{wildcards.sample}\\tPL:{params.platform}' {input.ref} {input.R1} {input.R2} | sentieon util sort -o ${{LOCAL}}/{wildcards.sample}.bwa.bam -t ${{THREADS}} --sam2bam -i -
	cp ${{LOCAL}}/{wildcards.sample}.bwa.bam* {wildcards.base}/{TIME}/{wildcards.sample}/	
	#######################
	"""
############
#       GenotypeFile
############
# Using Older version of samtools for this purpose
rule genotype:
	input:
		bam="{base}/{TIME}/{sample}/{sample}.{aligner}.bam",
		interval=config["genotypeBed"],
		ref=config["reference"],
		vcf2genotype=NGS_PIPELINE + "/scripts/vcf2genotype.pl",
		vcf2loh=NGS_PIPELINE + "/scripts/vcf2loh.pl",
	output:
		vcf="{base}/{TIME}/{sample}/calls/{sample}.{aligner}.samtools.vcf",
		gt= "{base}/{TIME}/{sample}/qc/{sample}.{aligner}.gt",
		loh="{base}/{TIME}/{sample}/qc/{sample}.{aligner}.loh"
	version: config["samtools"]
	params:
		rulename  = "genotype",
		batch     = config[config['host']]["job_genotype"],
		dest	  = config["genotypeDest"],
		host	  = config["host"]
	shell: """
	#######################
	module load samtools/{version}
	cp {input.bam} ${{LOCAL}}/
	samtools mpileup -u -C50 -f {input.ref} -l {input.interval} ${{LOCAL}}/{wildcards.sample}.{wildcards.aligner}.bam | bcftools view -gc - >${{LOCAL}}/{wildcards.sample}.{wildcards.aligner}.samtools.vcf
	perl {input.vcf2genotype} ${{LOCAL}}/{wildcards.sample}.{wildcards.aligner}.samtools.vcf >{output.gt}
	perl {input.vcf2loh} ${{LOCAL}}/{wildcards.sample}.{wildcards.aligner}.samtools.vcf >{output.loh}
	cp ${{LOCAL}}/{wildcards.sample}.{wildcards.aligner}.samtools.vcf {output.vcf}
	#######################
	"""
############
# Gender Estimation
# loh2gender=NGS_PIPELINE + "/scripts/guessGender.pl",
############
rule gender:
	input:
		loh="{base}/{TIME}/{sample}/qc/{sample}.bwa.loh",
		loh2gender=NGS_PIPELINE + "/scripts/guessGender.pl",
	output:
		"{base}/{TIME}/{sample}/qc/{sample}.gender"
	params:
		rulename  = "gender",
		batch     = config[config['host']]["job_default"],
	shell: """
	#######################
	perl {input.loh2gender} {input.loh} >{output}
	#######################
	"""
############
# Genotyping On Sample
############
rule patient_genotyping:
	input:
		gtFiles=lambda wildcards: SUB_GT[wildcards.subject],
		score=NGS_PIPELINE + "/scripts/scoreGenotyes.pl"
	output:
		"{subject}/{TIME}/qc/{subject}.genotyping.txt",
	params:
		rulename 	= "patient_genotyping",
		batch    	= config[config['host']]["job_default"],
		mail     	= config["mail"],
		mail_tool 	= NGS_PIPELINE + "/scripts/tsv2html.sh",
		host	 	= config["host"],
		diagnosis	= lambda wildcards: config['diagnosis'][wildcards.subject]
	shell: """
	#######################
	mkdir -p {wildcards.subject}/{TIME}/qc/GT
	mkdir -p {wildcards.subject}/{TIME}/qc/RATIO/
	cp {input.gtFiles} {wildcards.subject}/{TIME}/qc/GT/
	echo Sample >{wildcards.subject}/{TIME}/qc/RATIO/FirstColumn

	for file in {wildcards.subject}/{TIME}/qc/GT/*
	do
		sample=`basename ${{file}} .gt`
		echo ${{sample}} >>{wildcards.subject}/{TIME}/qc/RATIO/FirstColumn
		echo ${{sample}} >>{wildcards.subject}/{TIME}/qc/RATIO/${{sample}}.ratio
		for file2 in {wildcards.subject}/{TIME}/qc/GT/*
		do
			perl {input.score} ${{file}} ${{file2}} >>{wildcards.subject}/{TIME}/qc/RATIO/${{sample}}.ratio
		done
	done
	paste {wildcards.subject}/{TIME}/qc/RATIO/FirstColumn {wildcards.subject}/{TIME}/qc/RATIO/*.ratio >{wildcards.subject}/{TIME}/qc/{wildcards.subject}.genotyping.txt
	rm -rf {wildcards.subject}/{TIME}/qc/GT/ {wildcards.subject}/{TIME}/qc/RATIO/
	sed -i 's/Sample_//g' {wildcards.subject}/{TIME}/qc/{wildcards.subject}.genotyping.txt
	sed -i 's/.bwa//g' {wildcards.subject}/{TIME}/qc/{wildcards.subject}.genotyping.txt
	sed -i 's/.star//g' {wildcards.subject}/{TIME}/qc/{wildcards.subject}.genotyping.txt
	lines=$(cat {wildcards.subject}/{TIME}/qc/{wildcards.subject}.genotyping.txt|wc -l)
	if [ ${{lines}}  -gt 2 ] && [ ${{lines}}  -lt 50 ]; then
		ssh -q {params.host} "sh {params.mail_tool} --name {wildcards.subject} --diagnosis '{params.diagnosis}' --head {WORK_DIR}/{wildcards.subject}/{TIME}/qc/{wildcards.subject}.genotyping.txt | mutt -e \\\"my_hdr Content-Type: text/html\\\" -s 'Genotyping Result on {wildcards.subject}' `whoami`@mail.nih.gov {params.mail} "
	fi
	#######################
	"""
############
#       QC_Sum
############
rule qc_sum:
	input:
		TARGET,
		convertor = NGS_PIPELINE + "/scripts/makeQC.pl"
	output:
		"QC_AllSamples.txt"
	version: "v1.1"
	params:
		rulename = "qc_sum",
		batch    = config[config['host']]['job_default']
	shell: """
	#######################
	perl {input.convertor} `pwd` >{output}
	#######################
	"""
############
#       Samtools flagstat
############
rule flagsta:
	input:	"{base}/{TIME}/{sample}/{sample}.{aligner}.final.bam"
	output: "{base}/{TIME}/{sample}/qc/{sample}.{aligner}.flagstat.txt"
	version: config["samtools"]
	params:
		rulename  = "flagstat",
		batch     = config[config['host']]["job_flagstat"]
	shell: """
	#######################
	module load samtools/{version}
	samtools flagstat {input} > {output}
	#######################
	"""
############
#       Hotspot Coverage
############
rule hotspotcoverage:
	input:
		bam="{base}/{TIME}/{sample}/{sample}.{aligner}.final.bam",
		interval=lambda wildcards: config['hotspot_bed'][config['sample_captures'][wildcards.sample]],
		genome=config["reference"].replace(".fasta",".genome")
	output: "{base}/{TIME}/{sample}/qc/{sample}.{aligner}.hotspot.depth"
	version: config["bedtools"]
	params:
		rulename  = "hotspotcoverage",
		samtools  = config['samtools'],
		batch     = config[config['host']]["job_hotspot"],
	shell: """
	#######################
	module load samtools/{params.samtools}
	module load bedtools/2.22.0
	slopBed -i {input.interval} -g {input.genome} -b 50 >${{LOCAL}}/Region.bed
	samtools view -hF 0x400 -q 30 -L ${{LOCAL}}/Region.bed {input.bam} | samtools view -ShF 0x4 - | samtools view -SuF 0x200 - | bedtools coverage -abam - -b {input.interval} >{output}
	#######################
	"""
############
# Coverage
############
rule coverage:
	input:
		bam="{subject}/{TIME}/{sample}/{sample}.bwa.final.bam",
		bai="{subject}/{TIME}/{sample}/{sample}.bwa.final.bam.bai",
		interval= lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample]],
	output:
		"{subject}/{TIME}/{sample}/qc/{sample}.bwa.coverage.txt"
	version: config["bedtools"]
	params:
		rulename = "coverage",
		batch    = config[config['host']]["job_bedtools"]
	shell: """
	#######################
	module load bedtools/{version}
	bedtools coverage -abam {input.bam} -b {input.interval} -hist |grep "^all" > {output}
	#######################
	"""
############
# IGV Session file
############
rule igv_session:
	input: bams=lambda wildcards: SUB_IGV[wildcards.subject]
	output: "{subject}/{TIME}/igv/session_{subject}.xml"
	message: "Making IGV session xml file for {wildcards.subject}"
	params:
		rulename = "igv_session",
		batch    = config[config['host']]["job_covplot"],
		work_dir =  WORK_DIR
	shell: """
	#######################
	dir=`echo {params.work_dir} | sed -e 's/\/data\/MoCha/M:/g' `
	echo "<?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?>" >{output}
	echo "<Global genome=\\"hg19\\" locus=\\"\\" version=\\"3\\">" >>{output}
	echo "\t<Resources>" >>{output}
	for BAM in {input.bams}
	do
		bam=`echo "${{dir}}/${{BAM}}" |sed -e 's/\//\\\\\\/g'`
		echo "\t\t<Resource path=\\"${{bam}}\\"/>" >>{output}
	done
	echo "\t</Resources>" >>{output}
	echo "</Global>" >>{output}

	########
	dir=`echo {params.work_dir} | sed -e 's/data/Volumes/g' `
	echo "<?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?>" >{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml
        echo "<Global genome=\\"hg19\\" locus=\\"\\" version=\\"3\\">" >>{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml
        echo "\t<Resources>" >>{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml
        for BAM in {input.bams}
        do
                echo "\t\t<Resource path=\\"${{dir}}/${{BAM}}\\"/>" >>{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml
        done
        echo "\t</Resources>" >>{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml
        echo "</Global>" >>{wildcards.subject}/{wildcards.TIME}/igv/session_mac_{wildcards.subject}.xml	
	#######################
	"""
############
#       Picard Mark Duplicates
############
rule mark_dup:
	input:
		bam="{subject}/{TIME}/{sample}/{sample}.{base}.bam",
		bai="{subject}/{TIME}/{sample}/{sample}.{base}.bam.bai"

	output:
		bam=temp("{subject}/{TIME}/{sample}/{sample}.{base}.dd.bam"),
		index=temp("{subject}/{TIME}/{sample}/{sample}.{base}.dd.bam.bai"),
		metrics="{subject}/{TIME}/{sample}/qc/{base}.markdup.txt"
	version: config["picard"]
	params:
		rulename  = "mark_dup",
		batch     = config[config['host']]["job_markdup"],
		samtools  = config["samtools"],
		sentieon  = config["sentieon"],
	shell: """
	#######################
	module load picard/{version}
	module load {params.sentieon}
	echo "Using Sentieon"
	cp {input.bam} ${{LOCAL}}/
	cp {input.bai} ${{LOCAL}}/
	sentieon driver -t ${{THREADS}} -i ${{LOCAL}}/{wildcards.sample}.{wildcards.base}.bam --algo LocusCollector --fun score_info ${{LOCAL}}/{wildcards.base}.markdup.txt.tmp
	sentieon driver -t ${{THREADS}} -i ${{LOCAL}}/{wildcards.sample}.{wildcards.base}.bam --algo Dedup --score_info ${{LOCAL}}/{wildcards.base}.markdup.txt.tmp --metrics ${{LOCAL}}/{wildcards.base}.markdup.txt ${{LOCAL}}/{wildcards.sample}.{wildcards.base}.dd.bam
	cp ${{LOCAL}}/{wildcards.base}.markdup.txt {output.metrics}
	cp ${{LOCAL}}/{wildcards.sample}.{wildcards.base}.dd.bam {output.bam}
	cp ${{LOCAL}}/{wildcards.sample}.{wildcards.base}.dd.bam.bai {output.index}
	#######################
	"""
############
#       GATK Best Practices
############
rule gatk:
	input: 	bam="{base}/{TIME}/{sample}/{sample}.bwa.dd.bam",
		bai="{base}/{TIME}/{sample}/{sample}.bwa.dd.bam.bai",
		ref=config["reference"],
		phase1=config["1000G_phase1"],
		mills=config["Mills_and_1000G"]
	output:
		bam="{base}/{TIME}/{sample}/{sample}.bwa.final.bam",
		index="{base}/{TIME}/{sample}/{sample}.bwa.final.bam.bai",
	params:
		rulename  = "gatk",
		batch     = config[config['host']]["job_gatk"],
		sentieon  = config["sentieon"],
	shell: """
	#######################
	module load {params.sentieon}
	echo "Using Sentieon"
	sentieon driver --temp_dir ${{LOCAL}}/ -t ${{THREADS}} -r {input.ref} -i {input.bam} --algo Realigner -k {input.phase1} -k {input.mills} ${{LOCAL}}/{wildcards.sample}.lr.bam
	sentieon driver --temp_dir ${{LOCAL}}/ -t ${{THREADS}} -r {input.ref} -i ${{LOCAL}}/{wildcards.sample}.lr.bam --algo QualCal -k {input.phase1} -k {input.mills} ${{LOCAL}}/{wildcards.sample}.recalibration.matrix.txt
	sentieon driver --temp_dir ${{LOCAL}}/ -t ${{THREADS}} -r {input.ref} -i ${{LOCAL}}/{wildcards.sample}.lr.bam -q ${{LOCAL}}/{wildcards.sample}.recalibration.matrix.txt --algo QualCal -k {input.phase1} -k {input.mills} ${{LOCAL}}/{wildcards.sample}.recalibrationPost.matrix.txt --algo ReadWriter ${{LOCAL}}/{wildcards.sample}.bwa.final.bam
	cp ${{LOCAL}}/{wildcards.sample}.bwa.final.bam {output.bam}
	cp ${{LOCAL}}/{wildcards.sample}.bwa.final.bam.bai {output.index}

	sleep 60s
	touch {output.index}
	#######################
	"""
############
#       MuTect
############
rule mutect:
	input:
		lambda wildcards: somaticPairs[wildcards.Tumor],
		ref=config["reference"],
		dbsnp=config["dbsnp"],
		cosmic=config["cosmic"],
		interval=lambda wildcards: config['target_intervals'][pairedCapture[wildcards.Tumor]].replace("target","targetbp")
	output:
		vcf="{subject}/{TIME}/{Tumor}/calls/{Tumor}.MuTect.raw.vcf",
		call_stats="{subject}/{TIME}/{Tumor}/qc/{Tumor}.mutect.call_stats.txt",
		coverage="{subject}/{TIME}/{Tumor}/qc/{Tumor}.mutect.coverage.wig.txt"
	version: config["MuTect"]
	params:
		rulename = "mutect",
		R	 = config['version_R'],
		batch    = config[config['host']]["job_mutect"],
		normal   = lambda wildcards: config['matched_normal'][wildcards.Tumor][0],
		vcforder = NGS_PIPELINE + "/scripts/vcfOrderCol.R",
		sentieon =config["sentieon"],
		mt_sen   ="--max_normal_alt_frac 0.05 --max_normal_alt_cnt 4 --min_base_qual 20 ",
	shell: """
	#######################
	module load {params.sentieon} R
	echo "Using Sentieon"
	sentieon driver --temp_dir ${{LOCAL}}/\
		--read_filter MapQualFilter,min_map_qual=30 -t ${{THREADS}} -r {input.ref} \
		-i {input[2]} -i {input[0]} \
		--interval {input.interval} \
		--algo TNsnv --dbsnp {input.dbsnp} \
		--tumor_sample {wildcards.Tumor} --normal_sample {params.normal} \
		--call_stats_out {output.call_stats} --stdcov_out {output.coverage} {params.mt_sen} ${{LOCAL}}/{wildcards.Tumor}.vcf
	{params.vcforder} -i ${{LOCAL}}/{wildcards.Tumor}.vcf -o {output.vcf}
	#######################
	"""
############
#       MuTect2
############
rule mutect2:
	input:
		lambda wildcards: somaticPairs[wildcards.Tumor],
		ref=config["reference"],
		dbsnp=config["dbsnp"],
		cosmic=config["cosmic"],
		Twobit=config["reference"].replace('.fasta', '.2bit'),
		interval=lambda wildcards: config['target_intervals'][pairedCapture[wildcards.Tumor]].replace("target","targetbp")
	output:
		vcf="{subject}/{TIME}/{Tumor}/calls/{Tumor}.MuTect2.raw.vcf",
	params:
		rulename = "mutect2",
		batch    = config[config['host']]["job_mutect2"],
		normal   = lambda wildcards: config['matched_normal'][wildcards.Tumor][0],
		vcforder = NGS_PIPELINE + "/scripts/vcfOrderCol.R",
		sentieon =config["sentieon"],
		script2=NGS_PIPELINE+"/scripts/joinAdjacentSNPs.py",
	shell: """
	#######################
	module load sentieon-genomics/201808 R
	sentieon driver --temp_dir ${{LOCAL}}/\
		--read_filter MapQualFilter,min_map_qual=30 -t ${{THREADS}} -r {input.ref} \
		-i {input[2]} -i {input[0]} \
		--interval {input.interval} \
		--algo TNhaplotyper2 \
		--tumor_sample {wildcards.Tumor} --normal_sample {params.normal} ${{LOCAL}}/{wildcards.Tumor}.vcf
	sentieon tnhapfilter --tumor_sample {wildcards.Tumor}\
		--normal_sample {params.normal} -v ${{LOCAL}}/{wildcards.Tumor}.vcf\
		${{LOCAL}}/{wildcards.Tumor}.final.vcf
	{params.vcforder} -i ${{LOCAL}}/{wildcards.Tumor}.final.vcf -o ${{LOCAL}}/{wildcards.Tumor}.1.vcf
	module load samtools
	python {params.script2} -v ${{LOCAL}}/{wildcards.Tumor}.1.vcf -o ${{LOCAL}}/{wildcards.Tumor}.gold.vcf 1 {input[0]} {input.Twobit}
	cp ${{LOCAL}}/{wildcards.Tumor}.gold.vcf {output.vcf}
	cp ${{LOCAL}}/{wildcards.Tumor}.vcf {wildcards.subject}/{wildcards.TIME}/{wildcards.Tumor}/calls/{wildcards.Tumor}.MuTect2.vcf
	#######################
	"""
############
#	vcf2MAF on MuTect output
############
rule mutect2maf:
	input:
		vcf="{subject}/{TIME}/{sample}/calls/{sample}.{caller}.raw.vcf",
		ref=config["reference"]
	output:
		"{subject}/{TIME}/{sample}/calls/{sample}.{caller}.maf",
	version: config["vcf2maf"]
	params:
		rulename = "mutect2maf",
		isoforms = config['mskcc_isoforms'],
		batch    = config[config['host']]["job_vcf2maf"],
		VEP      = config['VEP'],
		script   = NGS_PIPELINE+"/scripts/filterMAF.pl",
		normal   = lambda wildcards: config['matched_normal'][wildcards.sample][0],
	shell: """
	#######################
	module load vcf2maf/{version} samtools VEP/{params.VEP}
	grep -P "#" {input.vcf} >${{LOCAL}}/input.vcf
	grep -P "\\tPASS\\t" {input.vcf} >>${{LOCAL}}/input.vcf
        vcf2maf.pl --input-vcf ${{LOCAL}}/input.vcf --output-maf ${{LOCAL}}/{wildcards.sample}.{wildcards.caller}.maf --tumor-id {wildcards.sample} --normal-id {params.normal} --ref-fasta {input.ref} --filter-vcf /fdb/VEP/89/cache/ExAC.r0.3.1.sites.vep.vcf.gz --vep-path  $VEPHOME --vep-forks ${{THREADS}} --vep-data $VEP_CACHEDIR --custom-enst {params.isoforms}
	cp ${{LOCAL}}/{wildcards.sample}.{wildcards.caller}.maf {wildcards.subject}/{TIME}/{wildcards.sample}/calls/{wildcards.sample}.{wildcards.caller}.raw.maf
        {params.script} ${{LOCAL}}/{wildcards.sample}.{wildcards.caller}.maf > {wildcards.subject}/{TIME}/{wildcards.sample}/calls/{wildcards.sample}.{wildcards.caller}.maf
	#######################
	"""
############
#       Strelka
############
rule strelka:
	input:
		lambda wildcards: somaticPairs[wildcards.Tumor],
		ref=config["reference"],
		interval=lambda wildcards: config['target_intervals'][pairedCapture[wildcards.Tumor]].replace("target","targetbp")
	output:
		snps="{subject}/{TIME}/{Tumor}/calls/{Tumor}.strelka.snvs.raw.vcf",
		indels="{subject}/{TIME}/{Tumor}/calls/{Tumor}.strelka.indels.raw.vcf"
	version: config["strelka"]
	params:
		rulename = "strelka",
		batch    = config[config['host']]["job_strelka"],
		config=NGS_PIPELINE + "/Tools_config/"+config["strelka_config"],
		vcftools = config["vcftools"]
	shell: """
	#######################
	module load strelka/{version}
	configureStrelkaWorkflow.pl --normal={input[2]} --tumor={input[0]}\
	--ref={input.ref} --config={params.config} --output-dir=${{LOCAL}}/strelka
	make -j ${{SLURM_CPUS_PER_TASK}} -f ${{LOCAL}}/strelka/Makefile
	module load vcftools/{params.vcftools}
	vcftools --vcf ${{LOCAL}}/strelka/results/passed.somatic.snvs.vcf --bed {input.interval} --out {output.snps} --recode --keep-INFO-all
	mv -f {output.snps}.recode.vcf {output.snps}
	vcftools --vcf ${{LOCAL}}/strelka/results/passed.somatic.indels.vcf --bed {input.interval}  --out {output.indels} --recode --keep-INFO-all
	mv -f {output.indels}.recode.vcf {output.indels}
	NORMAL=`basename {input[2]} .bwa.final.bam`
	sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t{wildcards.Tumor}/g" {output.snps}
	sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t{wildcards.Tumor}/g" {output.indels}

	#######################
	"""
